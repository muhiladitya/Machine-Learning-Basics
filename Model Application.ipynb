{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>                    Model Application</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from mlxtend.plotting import plot_linear_regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    " \n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"xtrain.csv\")\n",
    "df['question1'] = df['question1'].apply(lambda x: str(x))## converting other object types to str while using BOW\n",
    "df['question2'] = df['question2'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train-Test split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, df['is_duplicate'], test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_wordlen</th>\n",
       "      <th>q2_wordlen</th>\n",
       "      <th>common_words</th>\n",
       "      <th>total_words_union</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>2606</td>\n",
       "      <td>2606</td>\n",
       "      <td>2606</td>\n",
       "      <td>5178</td>\n",
       "      <td>5179</td>\n",
       "      <td>link origin account ea sport contact number speak</td>\n",
       "      <td>2nd year cs student average problem solve skil...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57228</th>\n",
       "      <td>57228</td>\n",
       "      <td>57228</td>\n",
       "      <td>57228</td>\n",
       "      <td>21983</td>\n",
       "      <td>79791</td>\n",
       "      <td>learn fast</td>\n",
       "      <td>learn minimal time</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.133631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8382</th>\n",
       "      <td>8382</td>\n",
       "      <td>8382</td>\n",
       "      <td>8382</td>\n",
       "      <td>16344</td>\n",
       "      <td>16345</td>\n",
       "      <td>think right bring child mess world</td>\n",
       "      <td>justified afraid bring child world</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60281</th>\n",
       "      <td>60281</td>\n",
       "      <td>60281</td>\n",
       "      <td>60281</td>\n",
       "      <td>105189</td>\n",
       "      <td>105447</td>\n",
       "      <td>german shepherd border collie mix look like</td>\n",
       "      <td>temperament border collie german shepherd mix</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48432</th>\n",
       "      <td>48432</td>\n",
       "      <td>48432</td>\n",
       "      <td>48432</td>\n",
       "      <td>86356</td>\n",
       "      <td>86357</td>\n",
       "      <td>year old work software engineer automotive fir...</td>\n",
       "      <td>old senior software engineer large tech compan...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1     id    qid1    qid2  \\\n",
       "2606         2606          2606   2606    5178    5179   \n",
       "57228       57228         57228  57228   21983   79791   \n",
       "8382         8382          8382   8382   16344   16345   \n",
       "60281       60281         60281  60281  105189  105447   \n",
       "48432       48432         48432  48432   86356   86357   \n",
       "\n",
       "                                               question1  \\\n",
       "2606   link origin account ea sport contact number speak   \n",
       "57228                                         learn fast   \n",
       "8382                  think right bring child mess world   \n",
       "60281        german shepherd border collie mix look like   \n",
       "48432  year old work software engineer automotive fir...   \n",
       "\n",
       "                                               question2  is_duplicate  \\\n",
       "2606   2nd year cs student average problem solve skil...             0   \n",
       "57228                                 learn minimal time             1   \n",
       "8382                  justified afraid bring child world             1   \n",
       "60281      temperament border collie german shepherd mix             0   \n",
       "48432  old senior software engineer large tech compan...             0   \n",
       "\n",
       "       q1_wordlen  q2_wordlen  common_words  total_words_union  similarity  \n",
       "2606           15          36             5                 46    0.000000  \n",
       "57228           5          10             3                 12    0.133631  \n",
       "8382           15          13             5                 23    0.000000  \n",
       "60281           9          10             0                 18    0.285714  \n",
       "48432          34          21             4                 51    0.166667  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Removing the ids and extra columns that dont add value in helping the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.drop([\"id\",\"Unnamed: 0\",\"Unnamed: 0.1\",\"is_duplicate\",\"qid1\",\"qid2\"],axis=1)##while splitting extra columns got added so we remove it\n",
    "X_test=X_test.drop([\"id\",\"Unnamed: 0\",\"Unnamed: 0.1\",\"is_duplicate\",\"qid1\",\"qid2\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                               question1  \\\n",
       " 2606   link origin account ea sport contact number speak   \n",
       " 57228                                         learn fast   \n",
       " 8382                  think right bring child mess world   \n",
       " 60281        german shepherd border collie mix look like   \n",
       " 48432  year old work software engineer automotive fir...   \n",
       " ...                                                  ...   \n",
       " 21243   property tax rate granville ohio compare georgia   \n",
       " 45891                                      control anger   \n",
       " 42613               stephen hawk work tv big bang theory   \n",
       " 43567                             video prove time video   \n",
       " 68268                              wake morning no place   \n",
       " \n",
       "                                                question2  q1_wordlen  \\\n",
       " 2606   2nd year cs student average problem solve skil...          15   \n",
       " 57228                                 learn minimal time           5   \n",
       " 8382                  justified afraid bring child world          15   \n",
       " 60281      temperament border collie german shepherd mix           9   \n",
       " 48432  old senior software engineer large tech compan...          34   \n",
       " ...                                                  ...         ...   \n",
       " 21243              accurate cognate rule spanish english          18   \n",
       " 45891                                      control anger           7   \n",
       " 42613          stephen hawk like work tv big bang theory          13   \n",
       " 43567                                   prove date video          21   \n",
       " 68268                              good way wake morning          15   \n",
       " \n",
       "        q2_wordlen  common_words  total_words_union  similarity  \n",
       " 2606           36             5                 46    0.000000  \n",
       " 57228          10             3                 12    0.133631  \n",
       " 8382           13             5                 23    0.000000  \n",
       " 60281          10             0                 18    0.285714  \n",
       " 48432          21             4                 51    0.166667  \n",
       " ...           ...           ...                ...         ...  \n",
       " 21243          10             0                 27    0.500000  \n",
       " 45891           8             5                 10    0.000000  \n",
       " 42613          14            10                 17    0.239046  \n",
       " 43567           9             5                 25    0.316228  \n",
       " 68268          11             5                 21    0.267261  \n",
       " \n",
       " [75000 rows x 7 columns],\n",
       "                                             question1  \\\n",
       " 3582                        zabbix server not run fix   \n",
       " 60498              parent let play video game weekday   \n",
       " 53227                            start program anchor   \n",
       " 21333                              confident positive   \n",
       " 3885                           good way decide career   \n",
       " ...                                               ...   \n",
       " 26543              substitute flour cornstarch recipe   \n",
       " 85764  not manufacture harvest large scale need human   \n",
       " 87585                            woman love enjoy sex   \n",
       " 32519                      good diet plan weight loss   \n",
       " 18831                     overcome anxiety depression   \n",
       " \n",
       "                                                question2  q1_wordlen  \\\n",
       " 3582                  proxy server refuse connection fix          12   \n",
       " 60498                            old daughter step house          15   \n",
       " 53227                                             anchor           8   \n",
       " 21333                           speak clarity confidence           8   \n",
       " 3885                                       decide career           9   \n",
       " ...                                                  ...         ...   \n",
       " 26543  recipe call cup flour want time recipe flour need           8   \n",
       " 85764     not manufacture harvest large scale human need          18   \n",
       " 87585                                    woman enjoy sex           6   \n",
       " 32519                            proper diet weight loss           9   \n",
       " 18831                                   overcome anxiety           7   \n",
       " \n",
       "        q2_wordlen  common_words  total_words_union  similarity  \n",
       " 3582           13             6                 19    0.000000  \n",
       " 60498          13             7                 21    0.000000  \n",
       " 53227           4             0                 12    0.161165  \n",
       " 21333           9             5                 12    0.000000  \n",
       " 3885            7             3                 13    0.104828  \n",
       " ...           ...           ...                ...         ...  \n",
       " 26543          25             2                 31    0.000000  \n",
       " 85764          13             7                 24    0.000000  \n",
       " 87585           4             3                  7    0.235702  \n",
       " 32519           9             5                 13    0.000000  \n",
       " 18831           7             2                 12    0.353553  \n",
       " \n",
       " [25000 rows x 7 columns])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorizing the text data by bag of words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 4430) (25000, 4430)\n"
     ]
    }
   ],
   "source": [
    "t=list(X_train[\"question1\"])\n",
    "\n",
    "vectorizer_bow_essay = CountVectorizer(min_df=10,max_features=5000)  #selecting top 5000 features\n",
    "vectorizer_bow_essay.fit(X_train[\"question1\"].astype('U').values)\n",
    "\n",
    "q1_one_hot_train= vectorizer_bow_essay.transform(t)\n",
    "q1_one_hot_test=vectorizer_bow_essay.transform(X_test[\"question1\"])\n",
    "\n",
    "\n",
    "print(q1_one_hot_train.shape,q1_one_hot_test.shape)##printing the shape check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 5000)\n"
     ]
    }
   ],
   "source": [
    "t1=list(X_train[\"question2\"])\n",
    "\n",
    "vectorizer_bow_essay = CountVectorizer(min_df=1,max_features=5000)  #selecting top 5000 features\n",
    "vectorizer_bow_essay.fit(t1)\n",
    "\n",
    "q2_one_hot_train= vectorizer_bow_essay.transform(t1)\n",
    "q2_one_hot_test=vectorizer_bow_essay.transform(X_test[\"question2\"])\n",
    "\n",
    "print(q2_one_hot_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Normalizing the numerical features</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train['q1_wordlen'].values.reshape(1,-1))## since it a 1D array we have to convert to a matrix\n",
    "\n",
    "q1_wordlen_train = normalizer.transform(X_train['q1_wordlen'].values.reshape(1,-1))\n",
    "q1_wordlen_test = normalizer.transform(X_test['q1_wordlen'].values.reshape(1,-1))\n",
    "\n",
    "q1_wordlen_train=q1_wordlen_train.T\n",
    "q1_wordlen_test=q1_wordlen_test.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train['q2_wordlen'].values.reshape(1,-1))\n",
    "\n",
    "q2_wordlen_train = normalizer.transform(X_train['q2_wordlen'].values.reshape(1,-1))\n",
    "q2_wordlen_test = normalizer.transform(X_test['q2_wordlen'].values.reshape(1,-1))\n",
    "\n",
    "q2_wordlen_train=q2_wordlen_train.T\n",
    "q2_wordlen_test=q2_wordlen_test.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train['common_words'].values.reshape(1,-1))\n",
    "\n",
    "common_words_train = normalizer.transform(X_train['common_words'].values.reshape(1,-1))\n",
    "common_words_test = normalizer.transform(X_test['common_words'].values.reshape(1,-1))\n",
    "\n",
    "common_words_train=common_words_train.T\n",
    "common_words_test=common_words_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train['total_words_union'].values.reshape(1,-1))\n",
    "\n",
    "total_words_union_train = normalizer.transform(X_train['total_words_union'].values.reshape(1,-1))\n",
    "total_words_union_test = normalizer.transform(X_test['total_words_union'].values.reshape(1,-1))\n",
    "\n",
    "total_words_union_train=total_words_union_train.T\n",
    "total_words_union_test=total_words_union_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train['similarity'].values.reshape(1,-1))\n",
    "\n",
    "similarity_train = normalizer.transform(X_train['similarity'].values.reshape(1,-1))\n",
    "similarity_test = normalizer.transform(X_test['similarity'].values.reshape(1,-1))\n",
    "\n",
    "similarity_train=similarity_train.T\n",
    "similarity_test=similarity_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 1)\n",
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(q2_wordlen_train.shape)\n",
    "print(q2_wordlen_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stacking the vectorized and normalized data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The datasets will be in csr after stacking \n",
    "X_tr=hstack((total_words_union_train,common_words_train,q2_wordlen_train,q1_wordlen_train,q2_one_hot_train,q1_one_hot_train,similarity_train))\n",
    "X_te=hstack((total_words_union_test,common_words_test,q2_wordlen_test,q1_wordlen_test,q2_one_hot_test,q1_one_hot_test,similarity_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Applying logistic regression</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparamenter tuning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed:   23.0s\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 133 out of 140 | elapsed:   24.9s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:   26.0s finished\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression(penalty=\"l2\",C=0.1)\n",
    "clf.fit(X_tr,y_train)\n",
    "\n",
    "parameters = [{'C': [0.1,0.001,0.0001,1, 10, 100, 1000], 'penalty':['l1', 'l2', 'elasticnet', 'none']}]\n",
    "\n",
    "grid_search=GridSearchCV(estimator = clf,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'neg_log_loss',\n",
    "                           cv = 5,\n",
    "                           verbose=20,## adding verbose just to know our progress\n",
    "                           n_jobs = -1)\n",
    "grid_search=grid_search.fit(X_tr, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5532082437754017"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_## in grid search it tries to maximize so we put scoring as negative log loss \n",
    "## so the answer we want is negative of the answer it displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5483662141955461\n"
     ]
    }
   ],
   "source": [
    "clf1=LogisticRegression(penalty=\"l2\",C=0.1)\n",
    "clf1.fit(X_tr,y_train)\n",
    "    \n",
    "sig_clf = CalibratedClassifierCV(clf1, method=\"isotonic\")## we are using clibratedclassifier because we need extact probabily of the class it belongs to \n",
    "sig_clf.fit(X_tr, y_train)\n",
    "\n",
    "predict_y =sig_clf.predict_proba(X_te)\n",
    "print(log_loss(y_test, predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5495825815986446\n"
     ]
    }
   ],
   "source": [
    "clf1=LogisticRegression(penalty=\"l2\",C=0.1)\n",
    "clf1.fit(X_tr,y_train)\n",
    "\n",
    "predict_y =clf.predict_proba(X_te)\n",
    "print(log_loss(y_test, predict_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is some difference in log loss values in the above implementaion of test dataset.This will be more has the dataset size increases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
